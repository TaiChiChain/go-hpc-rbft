// Copyright 2016-2017 Hyperchain Corp.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package txpool

import (
	"container/list"
	"crypto/md5"
	"encoding/binary"
	"encoding/hex"
	"time"

	"github.com/hyperchain/go-hpc-rbft/common/consensus"
	"github.com/hyperchain/go-hpc-rbft/common/metrics"
)

const (
	// DefaultBatchSize indicates the default value of batchSize
	DefaultBatchSize = 500
	// DefaultPoolSize indicates the default value of poolSize
	DefaultPoolSize = 50000
	// DefaultToleranceTime indicates the default value of tolerance out-of date time
	DefaultToleranceTime = 5 * time.Minute
	// DefaultBatchMaxMem indicates the default value of max mem batch size
	DefaultBatchMaxMem = 10 * 1024 * 1024 // 10MB
	// DefaultK indicates the default value of k
	DefaultK = 10

	// DefaultNamespace is the default namespace name.
	DefaultNamespace = "global"
)

// chainSupport is the support functions to check duplication and signature of request.
type chainSupport interface {
	// IsRequestsExist retrieves and returns if the given txs are included in ledger or not.
	IsRequestsExist(txs [][]byte) []bool
	// CheckSigns checks batch txs' signature validity.
	CheckSigns(txs [][]byte)
}

// TxPool contains all currently known transactions. Transactions
// enter the pool when they are received from other nodes or submitted
// locally. They are deleted from the pool when they are sure they have
// been included in ledger.
//
// The pool separates transactions being batched and transactions
// waiting to be batched. Transactions would be moved between those
// two states over time. A batch should be generated by consensus
// engine.
type TxPool[T any, Constraint consensus.TXConstraint[T]] interface {
	// GenerateRequestBatch generates a transaction batch and post it
	// to outside if there are transactions in txPool.
	GenerateRequestBatch() []*RequestHashBatch[T, Constraint]

	// AddNewRequests adds transactions into txPool.
	// When current node is primary, judge if we need to generate a batch by batch size.
	// When current node is backup, judge if we can eliminate some missing batches.
	// local indicates if this transaction is froward internally from RPC layer or not.
	// When we receive txs from other nodes(which have been added to other's tx pool) or
	// locally(API layer), we need to check duplication from ledger to avoid duplication
	// with committed txs on ledger.
	// Also, when we receive a tx locally, we need to check if these txs are out-of-date
	// time by time.
	AddNewRequests(txs [][]byte, isPrimary bool, local bool) ([]*RequestHashBatch[T, Constraint], []string)

	// RemoveBatches removes several batches by given digests of
	// transaction batches from the pool(batchedTxs).
	RemoveBatches(hashList []string)

	// IsPoolFull check if txPool is full which means if number of all cached txs
	// has exceeded the limited poolSize.
	IsPoolFull() bool

	// IsConfigBatch check if such batch is config-change batch or not
	IsConfigBatch(batchHash string) bool

	// HasPendingRequestInPool checks if there is non-batched tx(s) in tx pool or not
	HasPendingRequestInPool() bool

	// RestoreOneBatch moves one batch from batchStore back to non-batched txs.
	RestoreOneBatch(hash string) error

	// GetRequestsByHashList returns the transaction list corresponding to the given hash list.
	// When replicas receive hashList from primary, they need to generate a totally same
	// batch to primary generated one. deDuplicateTxHashes specifies some txs which should
	// be excluded from duplicate rules.
	// 1. If this batch has been batched, just return its transactions without error.
	// 2. If we have checked this batch and found we were missing some transactions, just
	//    return the same missingTxsHash as before without error.
	// 3. If one transaction in hashList has been batched before in another batch,
	//    return ErrDuplicateTx
	// 4. If we miss some transactions, we need to fetch these transactions from primary,
	//    and return missingTxsHash without error
	// 5. If this node get all transactions from pool, generate a batch and return its
	//    transactions without error
	GetRequestsByHashList(batchHash string, timestamp int64, hashList []string, deDuplicateTxHashes []string) (txs [][]byte, list []bool, missingTxsHash map[uint64]string, err error)

	// SendMissingRequests used by primary to find one batch in batchStore which should contain
	// txs which are specified in missingHashList.
	// 1. If there is no such batch, return ErrNoBatch.
	// 2. If there is such a batch, but it doesn't contain all txs in missingHashList,
	//    return ErrMismatch.
	// 3. If there is such a batch, and contains all needed txs, returns all needed txs by
	//    order.
	SendMissingRequests(batchHash string, missingHashList map[uint64]string) (txs map[uint64][]byte, err error)

	// ReceiveMissingRequests receives txs fetched from primary and add txs to txPool
	ReceiveMissingRequests(batchHash string, txs map[uint64][]byte) error

	// FilterOutOfDateRequests get the remained local txs in nonBatchedTxs which are
	// "out-of-date" by tolerance time.
	FilterOutOfDateRequests() ([][]byte, error)

	// RestorePool move all batched txs back to non-batched tx which should
	// only be used after abnormal recovery.
	RestorePool()

	// ReConstructBatchByOrder reconstruct batch from empty txPool by order, must be called after RestorePool.
	ReConstructBatchByOrder(oldBatch *RequestHashBatch[T, Constraint]) (deDuplicateTxHashes []string, err error)

	// Reset clears all cached txs in txPool and start with a pure empty environment,
	// except batches in saveBatches and local non-batched-txs that not included in ledger.
	Reset(saveBatches []string)

	// GetUncommittedTransactions returns all transactions that have not been committed to the ledger
	GetUncommittedTransactions(maxsize uint64) [][]byte

	// Start starts txPool service
	Start() error

	// Stop stops txPool service
	Stop()
}

// RequestHashBatch contains transactions that batched by primary.
type RequestHashBatch[T any, Constraint consensus.TXConstraint[T]] struct {
	BatchHash  string   // hash of this batch calculated by MD5
	TxHashList []string // list of all txs' hashes
	TxList     [][]byte // list of all txs
	LocalList  []bool   // list track if tx is received locally or not
	TimeList   []int64  // list track when the tx is received
	Timestamp  int64    // generation time of this batch
}

// request represents a wrapper of tx with receive timestamp, flag local indicates
// if this tx is received locally from RPC layer, for local txs, we need to check
// if this tx is old enough to re-broadcast it to other nodes.
// memory indicates the size of tx, which will be enabled when batchMemLimit is true
type request struct {
	tx        []byte
	local     bool
	timestamp int64
	memory    int
}

// Config defines the related params used by pool.
type Config struct {
	// K decides how long this checkpoint period is.
	K int

	// PoolSize is the max number of requests stored in pool, including batched and non-batched requests.
	PoolSize int

	// BatchSize is the max number of requests in one batch.
	BatchSize int

	// BatchMemLimit indicates whether limit batch mem size or not.
	BatchMemLimit bool

	// BatchMaxMem is the max memory size of one batch.
	BatchMaxMem uint

	// ToleranceTime is the max tolerance time duration of out-of-date requests.
	ToleranceTime time.Duration

	// MetricsProv is the metrics Provider used to generate metrics instance.
	MetricsProv metrics.Provider

	// Logger is used to print detailed log.
	Logger Logger
}

// txPoolImpl implements the TxPool interface
type txPoolImpl[T any, Constraint consensus.TXConstraint[T]] struct {
	// store all non-batched txs
	nonBatchedTxs *TxList

	// store all batches created by current primary in order, removed after
	// they are sure have been included in ledger.
	batchStore map[string]*RequestHashBatch[T, Constraint]

	// store batched txs' hash corresponding to batchStore
	batchedTxs map[string]bool

	// store missing txs' hash using missing batch's hash as key
	missingTxsRecord map[string]map[uint64]string

	// track the namespace which it belongs to
	namespace string

	// blockchain support
	support chainSupport

	// upper limit of txPool
	poolSize int

	// a batch can include how many transactions at most
	batchSize int

	// whether limit batch memory size or not
	batchMemLimit bool

	// the maximum memory size of one batch
	batchMaxMem int

	// the max tolerance time, if time.Now - timestamp > TOLERANCE_TIME, send event to consensus
	toleranceTime time.Duration

	// collect all related metrics
	metrics *txPoolMetrics

	// metricsProv is the metrics Provider used to generate metrics instance.
	metricsProv metrics.Provider

	logger Logger
}

// NewTxPool creates a new transaction pool
func NewTxPool[T any, Constraint consensus.TXConstraint[T]](namespace string, rlu chainSupport, config Config) TxPool[T, Constraint] {
	return newTxPoolImpl[T, Constraint](namespace, rlu, config)
}

// AddNewRequests adds transactions into txPool.
// When current node is primary, judge if we need to generate a batch by batch size.
// When current node is backup, judge if we can eliminate some missing batches.
// local indicates if this transaction is froward internally from RPC layer or not.
// When we receive txs from other nodes(which have been added to other's tx pool) or
// locally(API layer), we need to check duplication from ledger to avoid duplication
// with committed txs on ledger.
// Also, when we receive a tx locally, we need to check if these txs are out-of-date
// time by time.
func (pool *txPoolImpl[T, Constraint]) AddNewRequests(txs [][]byte, isPrimary bool, local bool) ([]*RequestHashBatch[T, Constraint], []string) {
	return pool.addNewRequests(txs, isPrimary, local)
}

// IsPoolFull check is txPool is full(including batched txs)
func (pool *txPoolImpl[T, Constraint]) IsPoolFull() bool {

	length := pool.nonBatchedTxs.Len() + len(pool.batchedTxs)
	return length >= pool.poolSize
}

// HasPendingRequestInPool checks if there is tx(s) in tx pool or not
func (pool *txPoolImpl[T, Constraint]) HasPendingRequestInPool() bool {

	length := pool.nonBatchedTxs.Len()
	return length > 0
}

// GenerateRequestBatch generates a transaction batch and post it to outside
// if there are transactions in txPool.
func (pool *txPoolImpl[T, Constraint]) GenerateRequestBatch() []*RequestHashBatch[T, Constraint] {
	return pool.generateTxBatch()
}

// GetRequestsByHashList returns the transaction list corresponding to the given hash list.
// When replicas receive hashList from primary, they need to generate a totally same
// batch to primary generated one. deDuplicateTxHashes specifies some txs which should
// be excluded from duplicate rules.
//  1. If this batch has been batched, just return its transactions without error.
//  2. If we have checked this batch and found we were missing some transactions, just
//     return the same missingTxsHash as before without error.
//  3. If one transaction in hashList has been batched before in another batch,
//     return ErrDuplicateTx
//  4. If we miss some transactions, we need to fetch these transactions from primary,
//     and return missingTxsHash without error
//  5. If this node get all transactions from pool, generate a batch and return its
//     transactions without error
func (pool *txPoolImpl[T, Constraint]) GetRequestsByHashList(batchHash string, timestamp int64, hashList []string, deDuplicateTxHashes []string) (txs [][]byte, localList []bool, missingTxsHash map[uint64]string, err error) {

	missingTxsHash = make(map[uint64]string)
	var hasMissing bool
	if batch, gErr := pool.getBatchByHash(batchHash); gErr == nil {
		// If replica already has this batch, directly return tx list
		pool.logger.Debugf("Replica already has this batch, batchHash: %s", batchHash)

		// If it's a config-change batch, replica turn into config-change mode
		if batch.IsConfBatch() {
			pool.logger.Infof("Replica found a config-change batch, batchHash: %s", batchHash)
		}

		txs = batch.TxList
		localList = batch.LocalList
		missingTxsHash = nil
		return
	}
	// If we have checked this batch and found we miss some transactions,
	// just return the same missingTxsHash as before
	if miss, ok := pool.missingTxsRecord[batchHash]; ok {
		txs = nil
		localList = nil
		missingTxsHash = miss
		return
	}

	timeList := make([]int64, len(hashList))
	deDuplicateMap := make(map[string]bool)
	for _, duplicateHash := range deDuplicateTxHashes {
		deDuplicateMap[duplicateHash] = true
	}
	for i, hash := range hashList {
		if deDuplicateMap[hash] {
			// ignore deDuplicate txs fro duplicate rule
			pool.logger.Noticef("Ignore de-duplicate tx %s when create same batch", hash)
		} else {
			_, ok := pool.batchedTxs[hash]
			if ok {
				// If this transaction has been batched, return ErrDuplicateTx
				pool.logger.Warningf("Duplicate transaction in getTxsByHashList with hash: %s, batch batchHash: %s, already batched", hash, batchHash)
				err = ErrDuplicateTx
				missingTxsHash = nil
				return
			}
		}
		if ok := pool.nonBatchedTxs.Has(hash); ok {
			req := pool.nonBatchedTxs.Get(hash).Value.(*request)
			timeList[i] = req.timestamp
			if !hasMissing {
				txs = append(txs, req.tx)
				localList = append(localList, req.local)
			}
		} else {
			pool.logger.Debugf("Can't find tx by hash: %s from txPool", hash)
			hasMissing = true
			missingTxsHash[uint64(i)] = hash
		}
	}

	// fetch missing txs if found missing txs from txPool
	if hasMissing {
		txs = nil
		localList = nil
		pool.missingTxsRecord[batchHash] = missingTxsHash
		return
	}
	batch := &RequestHashBatch[T, Constraint]{
		BatchHash:  batchHash,
		TxHashList: hashList,
		TxList:     txs,
		LocalList:  localList,
		TimeList:   timeList,
		Timestamp:  timestamp,
	}

	for _, hash := range hashList {
		e := pool.nonBatchedTxs.Get(hash)
		if e != nil {
			rErr := pool.nonBatchedTxs.Remove(e, hash)
			if rErr != nil {
				pool.logger.Warningf("Remove tx failed with err: %s", err)
			}
			pool.metrics.nonBatchedTxsNumber.Add(float64(-1))
		}

		_, ok := pool.batchedTxs[hash]
		if ok {
			pool.logger.Noticef("batchedTxs already add tx with hash %s", hash)
		} else {
			pool.batchedTxs[hash] = true
			pool.metrics.batchedTxsNumber.Add(float64(1))
		}
	}

	// If it's a config-change batch, replica turn into config-change mode
	if batch.IsConfBatch() {
		pool.logger.Infof("Replica found a config-change batch, batchHash: %s", batchHash)
	}
	pool.batchStore[batch.BatchHash] = batch
	pool.metrics.batchNumber.Add(float64(1))

	pool.logger.Debugf("Replica generate a transaction batch, which digest is %s, and now there are %d "+
		"pending txs and %d batches in txPool", batchHash, pool.nonBatchedTxs.Len(), len(pool.batchStore))
	missingTxsHash = nil
	return
}

// SendMissingRequests used by primary to find one batch in batchStore which should contain
// txs which are specified in missingHashList.
//  1. If there is no such batch, return ErrNoBatch.
//  2. If there is such a batch, but it doesn't contain all txs in missingHashList,
//     return ErrMismatch.
//  3. If there is such a batch, and contains all needed txs, returns all needed txs by
//     order.
func (pool *txPoolImpl[T, Constraint]) SendMissingRequests(batchHash string, missingHashList map[uint64]string) (txs map[uint64][]byte, err error) {

	txs = make(map[uint64][]byte, 0)
	// if this node doesn't have this batch, there is an error.
	batch, gErr := pool.getBatchByHash(batchHash)
	if gErr != nil {
		err = gErr
		txs = nil
		return
	}
	// If all transactions in missingHashList are in this batch, they should keep the order
	// So we can find them all by scanning this batch in order
	subBatchLen := uint64(len(batch.TxHashList))
	for i, hash := range missingHashList {
		if i >= subBatchLen || batch.TxHashList[i] != hash {
			txs = nil
			err = ErrMismatch
			return
		}
		txs[i] = batch.TxList[i]
	}
	return
}

// ReceiveMissingRequests receives txs fetched from primary and add txs to pool
func (pool *txPoolImpl[T, Constraint]) ReceiveMissingRequests(batchHash string, txs map[uint64][]byte) error {

	pool.logger.Debugf("Replica received %d missingTxs, batch hash: %s", len(txs), batchHash)
	validTxs := make([][]byte, 0, len(txs))
	if _, ok := pool.missingTxsRecord[batchHash]; !ok {
		pool.logger.Debug("Receive missing txs, but we don't miss this batch")
		return nil
	}
	for i, tx := range txs {
		missingHash, ok := pool.missingTxsRecord[batchHash][i]
		if !ok {
			// we would continuously receive transactions,
			// so that we should check the existence of missing tx at first.
			pool.logger.Debugf("Already received tx with index %d, ignore it", i)
			continue
		}
		txHash := requestHash[T, Constraint](tx)
		if txHash != missingHash {
			pool.logger.Warningf("Receive missing txs, but find a mismatch tx hash: %s", txHash)
			return ErrMismatch
		}
		validTxs = append(validTxs, tx)
		// delete missing record
		delete(pool.missingTxsRecord[batchHash], i)
	}

	pool.addMissingTxs(validTxs)
	if len(pool.missingTxsRecord[batchHash]) == 0 {
		delete(pool.missingTxsRecord, batchHash)
	}
	return nil
}

// RemoveBatches removes several batches by given digests of transaction
// batches from the pool(batchedTxs).
func (pool *txPoolImpl[T, Constraint]) RemoveBatches(hashList []string) {
	for _, batchHash := range hashList {
		batch, ok := pool.batchStore[batchHash]
		if ok {
			pool.logger.Debugf("Find batch %s in txPool batchStore which will be deleted "+
				"from txPool", batchHash)
			for _, txHash := range batch.TxHashList {
				delete(pool.batchedTxs, txHash)
				pool.metrics.batchedTxsNumber.Add(float64(-1))
			}
			delete(pool.batchStore, batchHash)
			pool.metrics.batchNumber.Add(float64(-1))
		} else {
			pool.logger.Warningf("Cannot find batch %s in txPool", batchHash)
		}
	}
	pool.logger.Debugf("Replica removes some batches in txPool, and now there are"+
		" %d batches in txPool", len(pool.batchStore))
}

// FilterOutOfDateRequests get the remained local txs in nonBatchedTxs which are
// "out-of-date" by tolerance time
func (pool *txPoolImpl[T, Constraint]) FilterOutOfDateRequests() ([][]byte, error) {

	pool.logger.Debugf("Before filter, there are %d pending txs in txPool", pool.nonBatchedTxs.Len())
	txs := make([][]byte, 0)
	i := 0
	var (
		localCount, deleteCount int
		n                       *list.Element
	)
	now := time.Now().UnixNano()
	nonBatchedLen := pool.nonBatchedTxs.Len()
	for e := pool.nonBatchedTxs.Front(); i < nonBatchedLen; e = n {
		element, ok := e.Value.(*request)
		if !ok {
			return nil, ErrInvalidTx
		}

		// get the next one then it can be removed
		n = e.Next()
		i++

		// for "out-of date" txs, if they are received locally, broadcast them to others
		// again, if they are received from others, directly delete them as it is other
		// nodes' responsibility to re-broadcast those txs, if they are duplicate txs,
		// directly delete them
		if now-element.timestamp > int64(pool.toleranceTime) {
			if element.local {
				// if the tx is duplicated, delete it
				exist := pool.support.IsRequestsExist([][]byte{element.tx})
				if exist[0] {
					pool.logger.Warningf("remove a duplicate transaction %s, already executed", requestHash[T, Constraint](element.tx))
					pool.metrics.duplicateTxsCounter.Add(float64(1))
					err := pool.nonBatchedTxs.Remove(e, requestHash[T, Constraint](element.tx))
					if err != nil {
						pool.logger.Warningf("Remove tx %s failed with err: %s", requestHash[T, Constraint](element.tx), err)
					}
					pool.metrics.nonBatchedTxsNumber.Add(float64(-1))
					//recallTx(element.tx)
					deleteCount++
					continue
				}

				// if the tx is received locally, broadcast it
				txs = append(txs, element.tx)
				localCount++
			} else {
				// if the tx is received from others, delete it
				err := pool.nonBatchedTxs.Remove(e, requestHash[T, Constraint](element.tx))
				if err != nil {
					pool.logger.Warningf("Remove tx %s failed with err: %s", requestHash[T, Constraint](element.tx), err)
				}
				pool.metrics.nonBatchedTxsNumber.Add(float64(-1))
				//recallTx(element.tx)
				deleteCount++
			}
		}

	}
	pool.logger.Debugf("After filter, there are %d pending txs in txPool, "+
		"%d out-of-date local txs will be re-broadcast, %d out-of-date remote txs have been deleted",
		pool.nonBatchedTxs.Len(), localCount, deleteCount)

	return txs, nil
}

// RestorePool move all batched txs back to non-batched tx which should only be used
// after abnormal recovery.
func (pool *txPoolImpl[T, Constraint]) RestorePool() {

	pool.logger.Debugf("Before restore pool, there are %d pending txs and %d batches in txPool",
		pool.nonBatchedTxs.Len(), len(pool.batchStore))

	for hash, batch := range pool.batchStore {
		for i := len(batch.TxList) - 1; i >= 0; i-- {
			req := pool.generateRequest(batch.TxList[i], batch.LocalList[i], batch.TimeList[i])
			txHash := requestHash[T, Constraint](req.tx)
			pool.nonBatchedTxs.PushFront(txHash, req)
			pool.metrics.nonBatchedTxsNumber.Add(float64(1))
			delete(pool.batchedTxs, txHash)
			pool.metrics.batchedTxsNumber.Add(float64(-1))
		}
		delete(pool.batchStore, hash)
		pool.metrics.batchNumber.Add(float64(-1))
	}

	// clear missingTxsRecord after abnormal.
	pool.missingTxsRecord = make(map[string]map[uint64]string)

	pool.logger.Debugf("After restore pool, there are %d pending txs and %d batches in txPool",
		pool.nonBatchedTxs.Len(), len(pool.batchStore))
}

// RestoreOneBatch move one batch in batchStore to txPool
func (pool *txPoolImpl[T, Constraint]) RestoreOneBatch(hash string) error {

	batch, err := pool.getBatchByHash(hash)
	if err != nil {
		pool.logger.Debug(err)
		return err
	}

	// remove from batchedTxs and batchStore
	for _, hash := range batch.TxHashList {
		delete(pool.batchedTxs, hash)
		pool.metrics.batchedTxsNumber.Add(float64(-1))
	}
	delete(pool.batchStore, hash)
	pool.metrics.batchNumber.Add(float64(-1))

	// put back into txPool
	for i := len(batch.TxList) - 1; i >= 0; i-- {
		req := pool.generateRequest(batch.TxList[i], batch.LocalList[i], batch.TimeList[i])
		pool.nonBatchedTxs.PushFront(requestHash[T, Constraint](req.tx), req)
		pool.metrics.nonBatchedTxsNumber.Add(float64(1))
	}

	pool.logger.Debugf("Replica removes one transaction batch, which hash is %s, and now there are "+
		"%d batches in txPool", hash, len(pool.batchStore))

	return nil
}

// ReConstructBatchByOrder reconstruct batch from empty txPool by order, must be called after RestorePool.
func (pool *txPoolImpl[T, Constraint]) ReConstructBatchByOrder(oldBatch *RequestHashBatch[T, Constraint]) (deDuplicateTxHashes []string, err error) {
	// check if there exists duplicate batch hash.
	if _, gErr := pool.getBatchByHash(oldBatch.BatchHash); gErr == nil {
		pool.logger.Warningf("When re-construct batch, batch %s already exists", oldBatch.BatchHash)
		err = ErrInvalidBatch
		return
	}

	// TxHashList has to match TxList by length and content
	if len(oldBatch.TxHashList) != len(oldBatch.TxList) {
		pool.logger.Warningf("Batch is invalid because TxHashList and TxList have different lengths.")
		err = ErrInvalidBatch
		return
	}

	for i, tx := range oldBatch.TxList {
		txHash := requestHash[T, Constraint](tx)
		if txHash != oldBatch.TxHashList[i] {
			pool.logger.Warningf("Batch is invalid because the hash %s in txHashList does not match "+
				"the calculated hash %s of the corresponding transaction.", oldBatch.TxHashList[i], txHash)
			err = ErrInvalidBatch
			return
		}
	}

	localList := make([]bool, len(oldBatch.TxHashList))
	timeList := make([]int64, len(oldBatch.TxHashList))
	for range oldBatch.TxHashList {
		localList = append(localList, false)
		timeList = append(timeList, 0)
	}

	batch := &RequestHashBatch[T, Constraint]{
		TxHashList: oldBatch.TxHashList,
		TxList:     oldBatch.TxList,
		LocalList:  localList,
		TimeList:   timeList,
		Timestamp:  oldBatch.Timestamp,
	}

	// The given batch hash should match with the calculated batch hash.
	batch.BatchHash = GetBatchHash[T, Constraint](batch)
	if batch.BatchHash != oldBatch.BatchHash {
		pool.logger.Warningf("The given batch hash %s does not match with the calculated batch hash %s.", oldBatch.BatchHash, batch.BatchHash)
		err = ErrInvalidBatch
		return
	}

	// There may be some duplicate transactions which are batched in different batches during vc, for those txs,
	// we only accept them in the first batch containing them and de-duplicate them in following batches.
	// NOTE! We don't need to filter txs from ledger here, as we only consider duplicate txs between
	// batches in pool.
	for _, txHash := range oldBatch.TxHashList {
		if _, ok := pool.batchedTxs[txHash]; ok {
			pool.logger.Noticef("De-duplicate tx %s when re-construct batch by order, already batched", txHash)
			deDuplicateTxHashes = append(deDuplicateTxHashes, txHash)
		} else {
			pool.batchedTxs[txHash] = true
			pool.metrics.batchedTxsNumber.Add(float64(1))
		}
	}

	for _, txHash := range oldBatch.TxHashList {
		if pool.nonBatchedTxs.Get(txHash) != nil {
			err := pool.nonBatchedTxs.Remove(pool.nonBatchedTxs.Get(txHash), txHash)
			if err != nil {
				pool.logger.Warningf("Remove tx failed with err: %s", err)
			}
			pool.metrics.nonBatchedTxsNumber.Add(float64(-1))
		}
	}

	pool.batchStore[batch.BatchHash] = batch
	pool.metrics.batchNumber.Add(float64(1))

	return
}

// Reset clears all cached txs in txPool and start with a pure empty environment,
// except batches in saveBatches and local non-batched-txs that not included in ledger.
func (pool *txPoolImpl[T, Constraint]) Reset(saveBatches []string) {
	localNonBatchedReqs := make([]*request, 0)
	localNonBatchedTxs := make([][]byte, 0)

	// clear remote non-batched-txs.
	for e := pool.nonBatchedTxs.Front(); e != nil; e = e.Next() {
		element, ok := e.Value.(*request)
		if !ok {
			pool.logger.Error("assert element failed")
			continue
		}

		if element.local {
			localNonBatchedReqs = append(localNonBatchedReqs, element)
			localNonBatchedTxs = append(localNonBatchedTxs, element.tx)
		} else {
			// clear remote request
			//recallTx(element.tx)
		}
	}

	// clear non-batched-txs that included in ledger
	pool.nonBatchedTxs = NewTxList()
	exists := pool.support.IsRequestsExist(localNonBatchedTxs)
	for i, exist := range exists {
		if exist {
			//recallTx(localNonBatchedReqs[i].tx)
		} else {
			pool.nonBatchedTxs.PushBack(requestHash[T, Constraint](localNonBatchedReqs[i].tx), localNonBatchedReqs[i])
		}
	}
	pool.metrics.nonBatchedTxsNumber.Set(float64(pool.nonBatchedTxs.Len()))

	saveBatchHashes := make(map[string]bool)
	for _, batchHash := range saveBatches {
		saveBatchHashes[batchHash] = true
	}

	// recall useless batched txs.
	for batchHash, batch := range pool.batchStore {
		// save useful batched txs.
		if saveBatchHashes[batchHash] {
			pool.logger.Debugf("save batch with digest %s", batchHash)
			continue
		}

		// clear batched txs if useless.
		for _, hash := range batch.TxHashList {
			delete(pool.batchedTxs, hash)
			//recallTx(batch.TxList[i])
		}
		delete(pool.batchStore, batchHash)
	}
	pool.metrics.batchedTxsNumber.Set(float64(len(pool.batchedTxs)))
	pool.metrics.batchNumber.Set(float64(len(pool.batchStore)))

	pool.missingTxsRecord = make(map[string]map[uint64]string)
}

// GetUncommittedTransactions returns all transactions that have not been committed to the ledger
func (pool *txPoolImpl[T, Constraint]) GetUncommittedTransactions(maxsize uint64) [][]byte {
	txs := make([][]byte, 0, maxsize)
	// get from non-batched txs
	i := uint64(0)
	for e := pool.nonBatchedTxs.Front(); e != nil && i < maxsize; e = e.Next() {
		element, ok := e.Value.(*request)
		if !ok {
			pool.logger.Error("assert element failed")
			continue
		}
		txs = append(txs, element.tx)
		i++
		if i >= maxsize {
			pool.logger.Error("The number of uncommitted transactions exceeds maxsize!")
			break
		}
	}
	// get from batched txs
	for _, batch := range pool.batchStore {
		for _, tx := range batch.TxList {
			if i < maxsize {
				txs = append(txs, tx)
				i++
			} else {
				break
			}
		}
		if i >= maxsize {
			pool.logger.Error("The number of uncommitted transactions exceeds maxsize!")
			break
		}
	}
	pool.logger.Infof("get %d uncommitted transactions from txpool", len(txs))
	return txs
}

// --------------------------------internal functions--------------------------------

// newTxPoolImpl creates a new transaction pool to gather, sort and filter inbound
// transactions from the network.
func newTxPoolImpl[T any, Constraint consensus.TXConstraint[T]](namespace string, support chainSupport, config Config) *txPoolImpl[T, Constraint] {
	poolSize := config.PoolSize
	if poolSize <= 0 {
		poolSize = DefaultPoolSize
	}

	batchSize := config.BatchSize
	if batchSize <= 0 {
		batchSize = DefaultBatchSize
	}

	batchMemLimit := config.BatchMemLimit

	maxMemSize := config.BatchMaxMem
	if maxMemSize <= 0 {
		maxMemSize = DefaultBatchMaxMem
	}

	tTime := config.ToleranceTime
	if tTime <= 0 {
		tTime = DefaultToleranceTime
	}

	k := config.K
	if k <= 0 {
		k = DefaultK
	}

	// as a primary in normal, it could store 2*k batches in txPool at most,
	// so that the size of txPool should be larger than 2*k times of a batch
	// if not, the consensus process may be blocked
	if batchSize*2*k > poolSize {
		config.Logger.Warningf("pool size %d is too small to fit batch size %d, when k is %d, "+
			"make pool size legal", poolSize, batchSize, k)
		// we would like to set a larger pool size if it's illegal
		poolSize = batchSize * 2 * 2 * k
	}

	txPool := &txPoolImpl[T, Constraint]{
		poolSize:      poolSize,
		batchSize:     batchSize,
		batchMemLimit: batchMemLimit,
		batchMaxMem:   int(maxMemSize),
		toleranceTime: tTime,
		support:       support,
		namespace:     namespace,
		metricsProv:   config.MetricsProv,
		logger:        config.Logger,
	}

	txPool.nonBatchedTxs = NewTxList()
	txPool.batchStore = make(map[string]*RequestHashBatch[T, Constraint])
	txPool.batchedTxs = make(map[string]bool)
	txPool.missingTxsRecord = make(map[string]map[uint64]string)

	txPool.logger.Infof("TxPool pool size = %d", poolSize)
	txPool.logger.Infof("TxPool batch size = %d", batchSize)
	txPool.logger.Infof("TxPool batch mem limit = %v", batchMemLimit)
	txPool.logger.Infof("TxPool batch max mem size = %d", int(maxMemSize))
	txPool.logger.Infof("TxPool tolerance time = %v", tTime)

	return txPool
}

// addNewRequests enqueues new transactions into the pool if they are not
// duplicate txs.
// for primary node, we need to check pending size and generate a batch once
// pending size has exceeded the batch size.
// for backup node, judge if we can eliminate some missing batches.
func (pool *txPoolImpl[T, Constraint]) addNewRequests(txs [][]byte, isPrimary bool, local bool) ([]*RequestHashBatch[T, Constraint], []string) {
	pool.metrics.incomingTxsCounter.Add(float64(len(txs)))
	// TODO(DH): move checkSign here
	validTxs := make([][]byte, 0, len(txs))
	validTxHashes := make([]string, 0, len(txs))
	for _, tx := range txs {
		txHash := requestHash[T, Constraint](tx)
		if _, ok := pool.batchedTxs[txHash]; ok {
			pool.logger.Warningf("Duplicate transaction found when addNewRequests with hash: %s in batched txs", txHash)
			pool.metrics.duplicateTxsCounter.Add(float64(1))
			//recallTx(tx)
			continue
		}
		if ok := pool.nonBatchedTxs.Has(txHash); ok {
			pool.logger.Warningf("Duplicate transaction when addNewRequests with hash: %s in non-batched txs", txHash)
			pool.metrics.duplicateTxsCounter.Add(float64(1))
			//recallTx(tx)
			continue
		}
		validTxs = append(validTxs, tx)
		validTxHashes = append(validTxHashes, txHash)
	}
	// check bloom filter to avoid duplicate with committed txs.
	ledgerExists := pool.support.IsRequestsExist(validTxs)
	for i, exist := range ledgerExists {
		if exist {
			pool.logger.Warningf("Duplicate transaction when addNewRequests with hash: %s in block chain", validTxHashes[i])
			pool.metrics.duplicateTxsCounter.Add(float64(1))
			//recallTx(validTxs[i])
			validTxs[i] = nil
		}
	}

	if isPrimary {
		// for primary, add validTxs to nonBatchedTxs and try to generate batch

		var hasConfigTx bool
		for i, tx := range validTxs {
			if tx != nil {
				req := pool.generateRequest(tx, local, time.Now().UnixNano())
				pool.nonBatchedTxs.PushBack(validTxHashes[i], req)
				pool.metrics.nonBatchedTxsNumber.Add(float64(1))
				if consensus.IsConfigTx[T, Constraint](tx) {
					hasConfigTx = true
				}
			}
		}

		if pool.nonBatchedTxs.Len() >= pool.batchSize {
			pool.logger.Debug("Reach batch size, generate a batch")
			return pool.generateTxBatch(), nil
		}

		if hasConfigTx {
			pool.logger.Debug("Receive a config tx, generate a config batch")
			return pool.generateTxBatch(), nil
		}

		return nil, nil
	}

	// for replica, add validTxs to nonBatchedTxs and check if the missing transactions and batches are fetched
	var completionMissingBatchHashes []string
	for i, tx := range validTxs {
		if tx != nil {
			req := pool.generateRequest(tx, local, time.Now().UnixNano())
			pool.nonBatchedTxs.PushBack(validTxHashes[i], req)
			pool.metrics.nonBatchedTxsNumber.Add(float64(1))
			// TODO(DH): refactor missingTxsRecord to map[string]uint64 if needed.
			for batchHash, missingTxsHash := range pool.missingTxsRecord {
				for j, missingTxHash := range missingTxsHash {
					// we have received a tx which we are fetching.
					if validTxHashes[i] == missingTxHash {
						// delete the missing record.
						delete(missingTxsHash, j)
					}
				}
				// we have completion one missing batch, notify consensus.
				if len(missingTxsHash) == 0 {
					delete(pool.missingTxsRecord, batchHash)
					completionMissingBatchHashes = append(completionMissingBatchHashes, batchHash)
				}
			}
		}
	}
	return nil, completionMissingBatchHashes
}

// generateTxBatch generates a transaction batch by batch limit (timeout or size).
func (pool *txPoolImpl[T, Constraint]) generateTxBatch() []*RequestHashBatch[T, Constraint] {

	poolLen := pool.nonBatchedTxs.Len()
	if poolLen == 0 {
		pool.logger.Debug("TxPool is empty")
		return nil
	}
	batches := pool.newTxBatch()
	if len(batches) == 0 {
		return nil
	}
	return batches
}

// newTxBatch creates one or more transaction batches by specified strategy.
// Batches will be generated from it might be a config batch or a normal one, the strategy as follow:
//  1. Normal case: Generate batches directly and they will be send out using a batch set.
//  2. A config tx: In one turn of generation, if we have found that there is a config tx,
//     generate a config batch and stuck txPool directly, so that only a config batch will be send out using
//     batch set after this turn of generation.
//  3. Memory Limit: Generate sub batches after txs picked out
func (pool *txPoolImpl[T, Constraint]) newTxBatch() []*RequestHashBatch[T, Constraint] {

	var (
		hashList  []string
		txList    [][]byte
		localList []bool
		timeList  []int64
	)
	// if txPool stores transactions more than batchSize, use the first batchSize transactions to generate a batch
	var batchSize int
	if poolLen := pool.nonBatchedTxs.Len(); poolLen > pool.batchSize {
		batchSize = pool.batchSize
	} else {
		batchSize = pool.nonBatchedTxs.Len()
	}

	i := 0
	var n *list.Element
	memLen := 0
	for e := pool.nonBatchedTxs.Front(); i < batchSize; e = n {
		req, ok := e.Value.(*request)
		if !ok {
			pool.logger.Error("the tx in nonBatch is nil")
			return nil
		}

		// Check the type of requests.
		// If there is a config tx, break directly and generate a config batch.
		if consensus.IsConfigTx[T, Constraint](req.tx) {
			hash := requestHash[T, Constraint](req.tx)
			pool.logger.Debugf("There is a config tx %s, generate a config batch directly, %d txs will be restored",
				hash, len(txList))

			// remove txs from batchedTxs
			for _, hash := range hashList {
				delete(pool.batchedTxs, hash)
				pool.metrics.batchedTxsNumber.Add(float64(-1))
			}

			// put txs back into txPool
			for k := len(txList) - 1; k >= 0; k-- {
				if localList != nil && timeList != nil {
					pushBackReq := pool.generateRequest(txList[k], localList[k], timeList[k])
					pool.nonBatchedTxs.PushFront(requestHash[T, Constraint](pushBackReq.tx), pushBackReq)
					pool.metrics.nonBatchedTxsNumber.Add(float64(1))
				}
			}

			// clear the elder slices
			txList = txList[0:0]
			hashList = hashList[0:0]
			localList = localList[0:0]
			timeList = timeList[0:0]

			// construct slices of a config batch
			txList = append(txList, req.tx)
			hashList = append(hashList, hash)
			localList = append(localList, req.local)
			timeList = append(timeList, req.timestamp)

			err := pool.nonBatchedTxs.Remove(e, hash)
			if err != nil {
				pool.logger.Warningf("Remove tx failed with err: %s", err)
			}
			pool.metrics.nonBatchedTxsNumber.Add(float64(-1))
			pool.batchedTxs[hash] = true
			pool.metrics.batchedTxsNumber.Add(float64(1))

			// jump out and directly generate a config hash batch
			break
		}

		if pool.batchMemLimit {
			memLen += req.memory
			if memLen > pool.batchMaxMem && len(txList) > 0 {
				// the memory size of batch cannot be larger than batchMaxMem, except there is only one tx in batch
				pool.logger.Debug("Reach the memory limit of batch, generate batch")
				break
			}
		}

		hash := requestHash[T, Constraint](req.tx)
		txList = append(txList, req.tx)
		hashList = append(hashList, hash)
		localList = append(localList, req.local)
		timeList = append(timeList, req.timestamp)

		i++
		n = e.Next()
		err := pool.nonBatchedTxs.Remove(e, hash)
		if err != nil {
			pool.logger.Warningf("Remove tx failed with err: %s", err)
		}
		pool.metrics.nonBatchedTxsNumber.Add(float64(-1))
		pool.batchedTxs[hash] = true
		pool.metrics.batchedTxsNumber.Add(float64(1))
	}

	// construct a hash batch
	var batches []*RequestHashBatch[T, Constraint]
	txBatch := &RequestHashBatch[T, Constraint]{
		TxHashList: hashList,
		TxList:     txList,
		LocalList:  localList,
		TimeList:   timeList,
		Timestamp:  time.Now().UnixNano(),
	}
	batchHash := GetBatchHash[T, Constraint](txBatch)
	txBatch.BatchHash = batchHash
	pool.batchStore[batchHash] = txBatch
	pool.metrics.batchNumber.Add(float64(1))

	// If it's a config batch, turn txPool into config state
	if txBatch.IsConfBatch() {
		pool.logger.Debugf("Primary generate a config batch with a ctx, which hash is %s, and now there are %d "+
			"pending txs and %d batches in txPool", batchHash, pool.nonBatchedTxs.Len(), len(pool.batchStore))
	} else {
		pool.logger.Debugf("Primary generate a transaction batch with %d txs, which hash is %s, and now there are %d "+
			"pending txs and %d batches in txPool", len(hashList), batchHash, pool.nonBatchedTxs.Len(), len(pool.batchStore))
	}

	batches = append(batches, txBatch)
	return batches
}

// addMissingTxs attempts to queue a batch of transactions into the pool after
// receiving missing txs from primary.
func (pool *txPoolImpl[T, Constraint]) addMissingTxs(txs [][]byte) {

	// record non-repeating txs and hash in txpool internally.
	innerValidTxs := make([][]byte, 0, len(txs))
	innerValidTxHashes := make([]string, 0, len(innerValidTxs))
	for _, tx := range txs {
		txHash := requestHash[T, Constraint](tx)

		if _, ok := pool.batchedTxs[txHash]; ok {
			pool.logger.Warningf("Duplicate transaction found when addMissingTxs "+
				"with hash: %s which has been batched", txHash)
			pool.metrics.duplicateTxsCounter.Add(float64(1))
			continue
		}

		if ok := pool.nonBatchedTxs.Has(txHash); ok {
			pool.logger.Warningf("Duplicate transaction when addMissingTxs with "+
				"hash: %s may be caused by receiving certain tx during fetching missing tx "+
				"from primary", txHash)
			pool.metrics.duplicateTxsCounter.Add(float64(1))
			continue
		}

		innerValidTxs = append(innerValidTxs, tx)
		innerValidTxHashes = append(innerValidTxHashes, txHash)
	}

	// for those txs fetched from primary, we need to check bloom filter to
	// avoid duplicate with committed txs.

	// record non-repeating txs and hash in ledger.
	outerValidTxs := make([][]byte, 0, len(innerValidTxs))
	outerValidTxHashes := make([]string, 0, len(innerValidTxs))
	ledgerExists := pool.support.IsRequestsExist(innerValidTxs)
	for i, exist := range ledgerExists {
		if exist {
			pool.logger.Warningf("Duplicate transaction when replicaAddNewTxs with hash: %s in block chain", innerValidTxHashes[i])
			pool.metrics.duplicateTxsCounter.Add(float64(1))
		} else {
			outerValidTxs = append(outerValidTxs, innerValidTxs[i])
			outerValidTxHashes = append(outerValidTxHashes, innerValidTxHashes[i])
		}
	}

	// start check sign immediately.
	pool.support.CheckSigns(outerValidTxs)

	for i, tx := range outerValidTxs {
		req := pool.generateRequest(tx, false, time.Now().UnixNano())
		pool.nonBatchedTxs.PushBack(outerValidTxHashes[i], req)
		pool.metrics.nonBatchedTxsNumber.Add(float64(1))
	}

	pool.logger.Debugf("Replica add transactions to non-batched txs, "+
		"and there are %d transactions in txPool", pool.nonBatchedTxs.Len())
}

// getBatchByHash find batch together with its index in batchStore, returns error
// if not found batchHash in batchStore.
func (pool *txPoolImpl[T, Constraint]) getBatchByHash(batchHash string) (*RequestHashBatch[T, Constraint], error) {

	batch, ok := pool.batchStore[batchHash]
	if ok {
		return batch, nil
	}

	return nil, ErrNoBatch
}

// GetBatchHash calculate hash of a RequestHashBatch
func GetBatchHash[T any, Constraint consensus.TXConstraint[T]](batch *RequestHashBatch[T, Constraint]) string {
	return CalculateMD5Hash(batch.TxHashList, batch.Timestamp)
}

// CalculateMD5Hash calculate hash by MD5
func CalculateMD5Hash(list []string, timestamp int64) string {
	h := md5.New()
	for _, hash := range list {
		_, _ = h.Write([]byte(hash))
	}
	if timestamp > 0 {
		b := make([]byte, 8)
		binary.LittleEndian.PutUint64(b, uint64(timestamp))
		_, _ = h.Write(b)
	}
	return hex.EncodeToString(h.Sum(nil))
}

func requestHash[T any, Constraint consensus.TXConstraint[T]](data []byte) string {
	tx, err := consensus.DecodeTx[T, Constraint](data)
	if err != nil {
		panic("decodeTx err in requestHash")
	}
	return Constraint(tx).RbftGetTxHash()
}

// IsConfigBatch check if such batch is config-change batch or not
func (pool *txPoolImpl[T, Constraint]) IsConfigBatch(batchHash string) bool {
	batch, err := pool.getBatchByHash(batchHash)
	if err != nil {
		return false
	}
	return batch.IsConfBatch()
}

func (pool *txPoolImpl[T, Constraint]) Start() error {
	var err error
	pool.metrics, err = newTxPoolMetrics(pool.metricsProv)
	if err != nil {
		pool.metrics.unregisterMetrics()
		pool.metrics = nil
		return err
	}
	return nil
}

func (pool *txPoolImpl[T, Constraint]) Stop() {
	if pool.metrics != nil {
		pool.metrics.unregisterMetrics()
	}
}

func (pool *txPoolImpl[T, Constraint]) generateRequest(tx []byte, local bool, timestamp int64) *request {
	if !pool.batchMemLimit {
		return &request{
			tx:        tx,
			local:     local,
			timestamp: timestamp,
		}
	}
	t, err := consensus.DecodeTx[T, Constraint](tx)
	if err != nil {
		pool.logger.Errorf("decode tx err in generateRequest: %v", err)
	}
	mem := Constraint(t).RbftGetSize()
	return &request{
		tx:        tx,
		local:     local,
		timestamp: timestamp,
		memory:    mem,
	}
}

func recallTx(tx *consensus.Transaction) {
	return
}
